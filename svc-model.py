# import librariesimport pandas as pdimport numpy as npimport pickleimport osfrom sklearn.svm import SVCfrom sklearn.model_selection import train_test_splitfrom imblearn.over_sampling import SMOTEfrom sklearn.model_selection import GridSearchCVfrom sklearn.metrics import confusion_matrix, precision_score, f1_score, accuracy_score, roc_auc_score, roc_curve, auc, recall_score# import the training datadata_raw = pd.read_excel('~/Desktop/master-thesis-code/data.xlsx', index_col=0)# set up lags for loopinglags = [3, 6, 9, 12, 18]# set up a params grid to find the best performing model# we can pass in each of the models# there is no need to redfine this for each modelparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],              'gamma': [0.01, 0.1, 1, 10, 100],              'kernel': ['rbf', 'linear']}# function to run svm modeldef run_svm_model(data, lag, test_size, scoring, params):        """This function takes the training data       and the lag as arguments and runs the model       each run creates a new model and the best one       is extracted vid grid search"""    # make a copy of the original DataFrame to avoid modifying it    data_copy = data.copy()    # initiliaze the logistic regression model    support_vector = SVC(random_state=42, verbose=1, probability=True)    # modify dataset for lag    # we want to set the recession indicator back by the lag so that t0 is aligned with t+lag    data_copy[f"nber_recession_{lag}_month_lag"] = data_copy['nber_recession'].shift(-lag)    # drop the original recession column and na values    data_copy = data_copy.drop(columns=['nber_recession'])    data_copy = data_copy.dropna()    # set up training and testing data    X = data_copy.drop(columns=[f"nber_recession_{lag}_month_lag"])    y = data_copy[f"nber_recession_{lag}_month_lag"]        # set up training and testing data    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)        # apply SMOTE only to the training data    smote = SMOTE(random_state=42)    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)        # set up the grid search object to perform the analysis    # set cross validation to 5 which is a standard benchmark    grid_search_cv = GridSearchCV(estimator=support_vector, param_grid=params, cv=5, scoring=scoring)        # perform the initial grid search    grid_search_cv.fit(X_train_resampled, y_train_resampled)        # get the best performing model    best_parameters = grid_search_cv.best_params_    best_model = grid_search_cv.best_estimator_        # predict the results    # set a custom higher threshold to reduce false positives    # the threshold of 0.5 is not bad, but producing too positive    # predicitions overall    # this returns two columns, one for the 0 classification    # and one for the 1 class, so we take all the rows    # from the second column     y_pred_raw = best_model.predict_proba(X_test)[:,1]        # set annual threshold    threshold = 0.60    # get classifications    y_pred = (y_pred_raw > threshold).astype(int)        # create a confusion matrix to visualize results    conf_mat = confusion_matrix(y_test, y_pred)    # get predicted values and metrics    metrics_obj= {       'accuracy': accuracy_score(y_test, y_pred),       'precision': precision_score(y_test, y_pred),       'recall': recall_score(y_test, y_pred),       'f1': f1_score(y_test, y_pred),       'roc_auc': roc_auc_score(y_test, y_pred),       }        return {'data': data_copy,            'best_parameters': best_parameters,            'best_model': best_model,            'y_true': y_test,            'y_pred': y_pred,            'confusion_matrix': conf_mat,            'model_metrics': metrics_obj}# make a list of the resulting logistic modelssvc_results = [(f"{lag}_month_lag_results", run_svm_model(data_raw, lag, 0.2, 'precision', param_grid)) for lag in lags]# write data to excel to transfer to local file# we will do further data processing in another scriptpath = '~/desktop/master-thesis-code/summary-svm-models.xlsx'writer = pd.ExcelWriter(path, engine='openpyxl')# make a dataframe of all accuracy resultsheaders_metrics = ['lag', 'accuracy', 'precision', 'recall', 'f1', 'roc_auc']# store the results for each iterationiteration_metrics = []# iterate over resultsfor result in svc_results:    # extract from the tuple    metrics = result[1]['model_metrics']    # extract each value    values = [val for _, val in metrics.items()]    # insert name of lag    values.insert(0, result[0])    # append to the list    iteration_metrics.append(values)# convert to a dataframemetric_data = pd.DataFrame(iteration_metrics, columns=headers_metrics)print(metric_data)metric_data.to_excel(writer, sheet_name='svc-summary-stats', index=False)# go through and see if the model is over or underestimating recessionsheaders_false_true_summary = ['lag', 'recession_true', 'recession_true_pred', 'recession_false', 'recession_false_pred', 'false_pos_rate', 'false_neg_rate']# store iteration calculationsiteration_summaries_lg = []# loop over datafor result in svc_results:    # extract the relevant data    data = result[1]    y_true_pred = pd.DataFrame({'y_actual': data['y_true'], 'y_predicted': data['y_pred']})        # create row of data with the calculations    true_pos = np.sum(y_true_pred['y_actual'] == 1)    true_neg = np.sum(y_true_pred['y_actual'] == 0)    pred_pos = np.sum(y_true_pred['y_predicted'] == 1)    false_pos_rate = np.sum((y_true_pred['y_actual'] == 0) & (y_true_pred['y_predicted'] == 1)) / (np.sum(y_true_pred['y_actual'] == 0))    false_neg_rate = np.sum((y_true_pred['y_actual'] == 1) & (y_true_pred['y_predicted'] == 0)) / (np.sum(y_true_pred['y_actual'] == 1))    # create a list of the stats to pass in    summary_stats = [true_pos, pred_pos, true_neg, len(y_true_pred) - pred_pos, false_pos_rate, false_neg_rate]        # insert lag name    summary_stats.insert(0, result[0])        # append to result list    iteration_summaries_lg.append(summary_stats)# convert to dfcomplete_summary_stats_lg = pd.DataFrame(iteration_summaries_lg, columns=headers_false_true_summary)# print resultsprint(complete_summary_stats_lg)# export summary statscomplete_summary_stats_lg.to_excel(writer, sheet_name='svc-pos-pred-data', index=False)# close writerwriter.close()# export the model for import into another file# define the export directoryexport_dir = "./model-exports/svc-models"# create directory if doesn't existos.makedirs(export_dir, exist_ok=True)# loop over the models and extract each of themfor lag, model_data in zip(lags, svc_results):    # write the model to binary and export it using pickle    with open(os.path.join(export_dir, f"svc-model-{lag}-month-lag.pkl"), "wb") as f:        # select the model and export        pickle.dump(model_data[1]['best_model'], f)