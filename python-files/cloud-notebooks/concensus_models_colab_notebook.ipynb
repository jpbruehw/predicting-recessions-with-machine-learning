{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvZ8p-gLMQgu"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import pickle\n",
        "import os\n",
        "import re\n",
        "#from sklearn.ensemble import GradientBoostingClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score, accuracy_score, roc_auc_score, roc_curve, auc, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create function to import models\n",
        "def get_exported_models(import_dir):\n",
        "    \"\"\"\n",
        "     This function takes the path to\n",
        "     the import directory\n",
        "     for each lagged model\n",
        "    \"\"\"\n",
        "    # initialize empty list to return\n",
        "    models = []\n",
        "    # loop over all the files in the import directory\n",
        "    # we can use the listdir() method on the os class\n",
        "    # this lists all the files and sub-directories\n",
        "    for file in os.listdir(import_dir):\n",
        "        # make sure the file name ends with .pkl\n",
        "        if file.endswith('.pkl'):\n",
        "            # get the file name\n",
        "            # this returns tuple of file name and extension\n",
        "            # so we pick [0] to get the file name\n",
        "            file_name = os.path.splitext(file)[0]\n",
        "            # read the file and append\n",
        "            # get full file path\n",
        "            # we need to do this to read from the pickle file\n",
        "            filepath = os.path.join(import_dir, file)\n",
        "            # call pickle to extract file\n",
        "            # we now use rb to read binary since we exported as bin\n",
        "            with open(filepath, \"rb\") as f:\n",
        "                model = pickle.load(f)\n",
        "            # append to list\n",
        "            models.append((file_name, model))\n",
        "    # return the list\n",
        "    return models"
      ],
      "metadata": {
        "id": "3wfBlzqWNsH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the folders to import the models and export results\n",
        "# colab folders/files only exist at runtime so we need to recreate\n",
        "# them everytime we disconnect and want to run the model again\n",
        "parent_dir_models = '/content/models'\n",
        "import_folders = ['gbc-models', 'logit-models', 'nn-models', 'ran-forest-models', 'svc-models']\n",
        "# loop and create\n",
        "for folder in import_folders:\n",
        "  folder_path = os.path.join(parent_dir_models, folder)\n",
        "  os.makedirs(folder_path, exist_ok=True)\n",
        "# create sep folder for data\n",
        "# we use this to store the summary\n",
        "# and threshold data which we will import\n",
        "parent_dir_other = '/content/'\n",
        "# create other directories we need\n",
        "other_dirs = ['data', 'cm-roc-plots', 'cm-shap-plots']\n",
        "# loop and create\n",
        "for folder in other_dirs:\n",
        "  folder_path = os.path.join(parent_dir_other, folder)\n",
        "  os.makedirs(folder_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "sLf4V3NoTAiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import all models\n",
        "#-----------------#\n",
        "\n",
        "# import logit models\n",
        "logit_import_dir = \"/content/models/logit-models\"\n",
        "# run and extract\n",
        "logit_models = get_exported_models(logit_import_dir)\n",
        "\n",
        "# import random forest models\n",
        "random_forest_import_dir = \"/content/models/ran-forest-models\"\n",
        "# run and extract\n",
        "random_forest_models = get_exported_models(random_forest_import_dir)\n",
        "\n",
        "# import the nn models\n",
        "nn_import_dir = \"/content/models/nn-models\"\n",
        "# run and extract\n",
        "nn_models = get_exported_models(nn_import_dir)\n",
        "\n",
        "# import the svc models\n",
        "svc_import_dir = \"/content/models/svc-models\"\n",
        "# run and extract\n",
        "svc_models = get_exported_models(svc_import_dir)\n",
        "\n",
        "# import gradient boosting models\n",
        "gradient_boosting_models = \"/content/models/gbc-models\"\n",
        "# run and extract\n",
        "gradient_boosting_models = get_exported_models(gradient_boosting_models)"
      ],
      "metadata": {
        "id": "g6vx2RRGOp3Y",
        "outputId": "93ea98de-b3d5-4876-a1ed-97eedef1c9c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.3.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.3.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "node array from the pickle has an incompatible dtype:\n- expected: [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]\n- got     : {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d4d2ccc51040>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrandom_forest_import_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/models/ran-forest-models\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# run and extract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrandom_forest_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_exported_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_forest_import_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# import the nn models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-72cccd2fc167>\u001b[0m in \u001b[0;36mget_exported_models\u001b[0;34m(import_dir)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# we now use rb to read binary since we exported as bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;31m# append to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msklearn/tree/_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree.__setstate__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32msklearn/tree/_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree._check_node_ndarray\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: node array from the pickle has an incompatible dtype:\n- expected: [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]\n- got     : {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import and load data\n",
        "#--------------------#\n",
        "\n",
        "# import raw data for testing\n",
        "data_raw = pd.read_excel('/content/data/data.xlsx', index_col=0)\n",
        "\n",
        "# import data to apply the correct threshold to the model\n",
        "model_summary = pd.read_excel('/content/data/all-summaries.xlsx')\n",
        "# extract the lag, threshold, and model name column\n",
        "threshold_lag_data = model_summary[['lag', 'threshold', 'model']]"
      ],
      "metadata": {
        "id": "mBhNBNjqRayw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to flatten list of nested lists\n",
        "# the predicted values are returned like this:\n",
        "# i.e. [[0],[1],[0],[1]]\n",
        "# we need to convert it to this:\n",
        "# i.e. [0,1,0,1]\n",
        "def flatten_list(matrix_list):\n",
        "    flat_list = []\n",
        "    for row in matrix_list:\n",
        "        flat_list.append(row[0])\n",
        "    return flat_list"
      ],
      "metadata": {
        "id": "U30InIUjZ_Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up helper function to extract the correct model\n",
        "# based on the lag, this takes the model list and the\n",
        "def extract_correct_lag_model(models, lag):\n",
        "    \"\"\"This function take models list\n",
        "       and the lag to match\"\"\"\n",
        "    # create regex pattern based on lag\n",
        "    lag_match_pattern = rf\"{lag}-month-lag\"\n",
        "    # loop over models\n",
        "    for model in models:\n",
        "        # use .search() method to check the entire string\n",
        "        # check the first part of tuple for name\n",
        "        # then return second part with the model\n",
        "        if re.search(lag_match_pattern, model[0]):\n",
        "            return model[1]\n",
        "    # if no model is found return none\n",
        "    # this should never be triggered but good\n",
        "    # to include just in case\n",
        "    return None"
      ],
      "metadata": {
        "id": "FfrlkWTrR8W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create helper function to get the correct threshold for the given lag\n",
        "def extract_correct_threshold(model, data, lag):\n",
        "    \"\"\"This function takes a dataframe\n",
        "       of the threshold data and lags\n",
        "       as well as the model name\"\"\"\n",
        "    # get the correct entry\n",
        "    entry = data[(data['lag'] == lag) & (data['model'] == model)]\n",
        "    # get the threshold\n",
        "    # access the value threshold from column to extract\n",
        "    # the raw number, only one val so we access 1st index\n",
        "    threshold_value = entry['threshold'].values[0]\n",
        "    return threshold_value"
      ],
      "metadata": {
        "id": "RAtWRS26SBYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create helper function to make vote classification\n",
        "# this will be applied to each row in the prediction dataframe\n",
        "# if the sum is greater than or equal to 3, we assign a model vote\n",
        "# of 1, otherwise 0\n",
        "def assign_vote(row):\n",
        "    # check row sum\n",
        "    if row.sum() >= 3:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "ugnzAxhCSEwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up lags for looping\n",
        "lags = [3, 6, 9, 12, 18]"
      ],
      "metadata": {
        "id": "abfy4E-1SHo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the models are already trained so we just need to test the models\n",
        "# create function to run\n",
        "def run_concensus_model(data, lag, test_size, summary_data):\n",
        "    \"\"\"This model accepts testing data and a lag\n",
        "       it then uses a simple voting concensus to determine\n",
        "       class, i.e. if a majority of models say 1,\n",
        "       then the model returns 1\"\"\"\n",
        "\n",
        "    # make a copy of the original DataFrame to avoid modifying it\n",
        "    data_copy = data.copy()\n",
        "\n",
        "    # modify dataset for lag\n",
        "    # we want to set the recession indicator back by the lag so that t0 is aligned with t+lag\n",
        "    data_copy[f\"nber_recession_{lag}_month_lag\"] = data_copy['nber_recession'].shift(-lag)\n",
        "\n",
        "    # drop the original recession column and na values\n",
        "    data_copy = data_copy.drop(columns=['nber_recession'])\n",
        "    data_copy = data_copy.dropna()\n",
        "\n",
        "    # set up training and testing data\n",
        "    X = data_copy.drop(columns=[f\"nber_recession_{lag}_month_lag\"])\n",
        "    y = data_copy[f\"nber_recession_{lag}_month_lag\"]\n",
        "\n",
        "    # set up training and testing data\n",
        "    # we don't need the trainin data in this case, only the testing data\n",
        "    _, X_test, _, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "    # extract the relevant model for the given lag\n",
        "    random_forest = extract_correct_lag_model(random_forest_models, lag)\n",
        "    svc = extract_correct_lag_model(svc_models, lag)\n",
        "    gradient_boosting = extract_correct_lag_model(gradient_boosting_models, lag)\n",
        "    neural_network = extract_correct_lag_model(nn_models, lag)\n",
        "    logit_model = extract_correct_lag_model(logit_models, lag)\n",
        "\n",
        "    # get the threshold for the relevant lag\n",
        "    rf_threshold = extract_correct_threshold('rf', summary_data, lag)\n",
        "    svc_threshold = extract_correct_threshold('svc', summary_data, lag)\n",
        "    gbc_threshold = extract_correct_threshold('gbc', summary_data, lag)\n",
        "    nn_threshold = extract_correct_threshold('nn', summary_data, lag)\n",
        "    logit_threshold = extract_correct_threshold('logit', summary_data, lag)\n",
        "\n",
        "    # make the calculations for each model\n",
        "    rand_forest_pred = (random_forest.predict_proba(X_test)[:,1] > rf_threshold).astype(int)\n",
        "    svc_pred = (svc.predict_proba(X_test)[:,1] > svc_threshold).astype(int)\n",
        "    gbc_pred = (gradient_boosting.predict_proba(X_test)[:,1] > gbc_threshold).astype(int)\n",
        "    logit_pred = (logit_model.predict_proba(X_test)[:,1] > logit_threshold).astype(int)\n",
        "\n",
        "    # neural network needs to handled a bit differently\n",
        "    # we first predict directly, then flatten list\n",
        "    # of lists which it returns\n",
        "    nn_predictions_prob_raw = neural_network.predict(X_test)\n",
        "    # flatten list of lists\n",
        "    nn_predictions_prob = flatten_list(nn_predictions_prob_raw)\n",
        "    nn_pred = (nn_predictions_prob > nn_threshold).astype(int)\n",
        "\n",
        "    # create a dataframe of the concensus predictions\n",
        "    model_votes = pd.DataFrame({\n",
        "    'rand_forest_pred': rand_forest_pred,\n",
        "    'svc_pred': svc_pred,\n",
        "    'gbc_pred': gbc_pred,\n",
        "    'nn_pred': nn_pred,\n",
        "    'logit_pred': logit_pred\n",
        "    })\n",
        "\n",
        "    # use simple voting system to dtermine whether to make a positive classfication\n",
        "    # if three or more models give a positive classification, then 1 otherwise 0\n",
        "    # we use the apply fumction to then use the voting function across each row\n",
        "    model_votes['concensus_vote'] = model_votes.apply(assign_vote, axis=1)\n",
        "\n",
        "    # now we can comparet the accuracy of the concensus model\n",
        "    y_pred = model_votes['concensus_vote'].values\n",
        "\n",
        "    # create a confusion matrix to visualize results\n",
        "    conf_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # get predicted values and metrics\n",
        "    metrics_obj= {\n",
        "       'accuracy': accuracy_score(y_test, y_pred),\n",
        "       'precision': precision_score(y_test, y_pred),\n",
        "       'recall': recall_score(y_test, y_pred),\n",
        "       'f1': f1_score(y_test, y_pred),\n",
        "       'roc_auc': roc_auc_score(y_test, y_pred),\n",
        "       }\n",
        "\n",
        "    # return summary output\n",
        "    return {'data': data_copy,\n",
        "            'model_votes': model_votes,\n",
        "            'y_true': y_test,\n",
        "            'predicted_vals_binary': y_pred,\n",
        "            'confusion_matrix': conf_mat,\n",
        "            'model_metrics': metrics_obj}"
      ],
      "metadata": {
        "id": "y4ue6pHySKR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the model for each lag\n",
        "concensus_results = [(f\"{lag}_month_lag_results\", run_concensus_model(data_raw, lag, 0.2, threshold_lag_data)) for lag in lags]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdvSpcS1SOmj",
        "outputId": "3fcec7d0-afe9-4135-ad97-358f6d0986f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
            "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=12)]: Done  50 out of  50 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
            "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=12)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 4ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
            "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=12)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
            "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
            "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a dataframe of all accuracy results\n",
        "headers_metrics = ['lag', 'accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'conf_matrix']\n",
        "# store the results for each iteration\n",
        "iteration_metrics = []\n",
        "# iterate over results\n",
        "for result in concensus_results:\n",
        "    # extract from the tuple\n",
        "    metrics = result[1]['model_metrics']\n",
        "    # extract each value\n",
        "    values = [val for _, val in metrics.items()]\n",
        "    # insert name of lag\n",
        "    values.insert(0, result[0])\n",
        "    # get the confusion matric\n",
        "    conf_matrix = result[1]['confusion_matrix']\n",
        "    # append to values\n",
        "    values.append(conf_matrix)\n",
        "    # append to the list\n",
        "    iteration_metrics.append(values)\n",
        "# convert to a dataframe\n",
        "metric_data = pd.DataFrame(iteration_metrics, columns=headers_metrics)\n",
        "\n",
        "print(metric_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD2XohNDf52H",
        "outputId": "0921ed0a-4076-47e9-850d-82df1688201a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    lag  accuracy  precision    recall        f1   roc_auc  \\\n",
            "0   3_month_lag_results  0.952703   0.733333  0.785714  0.758621  0.877932   \n",
            "1   6_month_lag_results  0.939189   0.666667  0.800000  0.727273  0.877444   \n",
            "2   9_month_lag_results  0.904762   0.619048  0.684211  0.650000  0.810855   \n",
            "3  12_month_lag_results  0.890411   0.523810  0.647059  0.578947  0.784770   \n",
            "4  18_month_lag_results  0.910345   0.681818  0.714286  0.697674  0.828917   \n",
            "\n",
            "            conf_matrix  \n",
            "0   [[130, 4], [3, 11]]  \n",
            "1   [[127, 6], [3, 12]]  \n",
            "2   [[120, 8], [6, 13]]  \n",
            "3  [[119, 10], [6, 11]]  \n",
            "4   [[117, 7], [6, 15]]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# go through and see if the model is over or underestimating recessions\n",
        "headers_false_true_summary = ['lag', 'recession_true', 'recession_true_pred', 'recession_false', 'recession_false_pred', 'false_pos_rate', 'false_neg_rate']\n",
        "\n",
        "# store iteration calculations\n",
        "iteration_summaries = []\n",
        "\n",
        "# loop over data\n",
        "for result in concensus_results:\n",
        "    # extract the relevant data\n",
        "    data = result[1]\n",
        "    y_true_pred = pd.DataFrame({'y_actual': data['y_true'], 'y_predicted': data['predicted_vals_binary']})\n",
        "\n",
        "    # create row of data with the calculations\n",
        "    true_pos = np.sum(y_true_pred['y_actual'] == 1)\n",
        "    true_neg = np.sum(y_true_pred['y_actual'] == 0)\n",
        "    pred_pos = np.sum(y_true_pred['y_predicted'] == 1)\n",
        "    false_pos_rate = np.sum((y_true_pred['y_actual'] == 0) & (y_true_pred['y_predicted'] == 1)) / (np.sum(y_true_pred['y_actual'] == 0))\n",
        "    false_neg_rate = np.sum((y_true_pred['y_actual'] == 1) & (y_true_pred['y_predicted'] == 0)) / (np.sum(y_true_pred['y_actual'] == 1))\n",
        "\n",
        "    # create a list of the stats to pass in\n",
        "    summary_stats = [true_pos, pred_pos, true_neg, len(y_true_pred) - pred_pos, false_pos_rate, false_neg_rate]\n",
        "\n",
        "    # insert lag name\n",
        "    summary_stats.insert(0, result[0])\n",
        "\n",
        "    # append to result list\n",
        "    iteration_summaries.append(summary_stats)\n",
        "\n",
        "# convert to df\n",
        "complete_summary_stats = pd.DataFrame(iteration_summaries, columns=headers_false_true_summary)\n",
        "\n",
        "# print results\n",
        "print(complete_summary_stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98fWolIJgIwy",
        "outputId": "e8ddfcb6-9dd0-48e0-c2b2-b1abf1b59632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    lag  recession_true  recession_true_pred  recession_false  \\\n",
            "0   3_month_lag_results              14                   15              134   \n",
            "1   6_month_lag_results              15                   18              133   \n",
            "2   9_month_lag_results              19                   21              128   \n",
            "3  12_month_lag_results              17                   21              129   \n",
            "4  18_month_lag_results              21                   22              124   \n",
            "\n",
            "   recession_false_pred  false_pos_rate  false_neg_rate  \n",
            "0                   133        0.029851        0.214286  \n",
            "1                   130        0.045113        0.200000  \n",
            "2                   126        0.062500        0.315789  \n",
            "3                   125        0.077519        0.352941  \n",
            "4                   123        0.056452        0.285714  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up writer to export the summary stats\n",
        "path = '/content/concensus-summary.xlsx'\n",
        "writer = pd.ExcelWriter(path, engine='openpyxl')"
      ],
      "metadata": {
        "id": "qsy9p5Zhg52O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export to excel\n",
        "metric_data.to_excel(writer, sheet_name='summary_stats', index=False)"
      ],
      "metadata": {
        "id": "LCEO20cVhxDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add summary stats to excel output\n",
        "complete_summary_stats.to_excel(writer, sheet_name='pos_neg_acc_summary', index=False)\n",
        "# close writer\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "745B8db3h1Oe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}