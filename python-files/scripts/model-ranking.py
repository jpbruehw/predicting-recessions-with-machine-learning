# import packagesimport pandas as pdimport numpy as npfrom scipy.stats import kruskalfrom itertools import combinationsfrom scipy.stats import wilcoxon# import the sheetsummary_stats = pd.read_excel('~/desktop/master-thesis-code/model-summaries/all-summaries.xlsx')# group by the lag to test at different levelslag_stats = summary_stats.groupby('lag')# set up lagslags = [3, 6, 9, 12, 18]# set up writer tpo exportpath = '~/desktop/master-thesis-code/model-summaries/model-ranking.xlsx'writer = pd.ExcelWriter(path, engine='openpyxl')# metics to consider for rankingmetrics = ['f1', 'recall', 'precision', 'accuracy', 'roc_auc']# setup result containerresults = []# loop over and perform resultsfor lag in lags:        # extract the grouped results for lag    lag_data = lag_stats.get_group(lag)        # set up dictionary to store ranks    # this creates list we can use as the key for the     lag_result = {'lag': [lag] * len(lag_data)}        # loop over each metric    for metric in metrics:        # sort the data based on the current metric        sorted_rank = lag_data[['model', metric]].sort_values(by=metric, ascending=False)        # store the results        lag_result[f'{metric}_model_ranking'] = sorted_rank['model'].tolist()        lag_result[f'{metric}_values'] = sorted_rank[metric].tolist()        # convert to dataframe        lag_result_df = pd.DataFrame(lag_result)        # set index as the lag        lag_result_df = lag_result_df.set_index('lag')        # append the lag_result to results DataFrame    results.append(lag_result_df)# concat the dateframes and exportall_results = pd.concat(results)all_results.to_excel(writer, sheet_name='all-comparisons')# look at the average for each model vs. the logit model#------------------------------------------------------## get list of the modelsall_models = np.unique(summary_stats['model'])# remove the logit model from the model listml_models = [model for model in all_models if model != 'logit']# loop over the models and compare each one to the logit stats# helper function to compare two models overall# regardless of the lag using all metricsdef compare_two_models(model_1, model_2):    """Function to find the scores for each       model than compare them using the Kurskal-Wallis       test to see if there is significant       difference between them"""           # get the metrics for model_1    model_1_metrics = summary_stats.loc[summary_stats['model'] == model_1, metrics]    # flatten to single list    model_1_metrics = model_1_metrics.values.flatten()    # get the metrics for model two    model_2_metrics = summary_stats.loc[summary_stats['model'] == model_2, metrics]    model_2_metrics = model_2_metrics.values.flatten()    # now perform the kruskal-wallis test    h_statistic, p_value_kw = kruskal(model_1_metrics, model_2_metrics)    # perform the wilcoxon-signed rank test    statistic, p_value_ws = wilcoxon(model_1_metrics, model_2_metrics, zero_method='pratt')    # return the values as dictionary    result = {'kw_result': {'h_statistic': h_statistic, 'p_value_kw': p_value_kw},              'ws': {'statistic': statistic, 'p_value_ws': p_value_ws},'models': {model_1, model_2}}    # return the result    return result# create all unique combinations of modelsall_combos = list(combinations(all_models, 2))# perform calculations on each combogeneral_performance = [compare_two_models(combo[0], combo[1]) for combo in all_combos]# now find results that are significant in generalsignificant_gen_results = [result for result in general_performance                           if result['kw_result']['p_value_kw'] < 0.05 or result['kw_result']['p_value_kw'] < 0.05]# both wilcoxon and kruskal-wallis seem to be reporting the same results# initialize list to store significant results for each lagsignificant_results_by_lag = []# initialize list to store all results for each laglag_sig_test_results = []# loop over the grouped data# since we grouped the data, we can# loop over the group identifier and the# the corresponding data directly# https://stackoverflow.com/questions/27405483/how-to-loop-over-grouped-pandas-dataframefor lag, lag_data in lag_stats:    # get the data for the current lag    lag_data_sig_test = lag_stats.get_group(lag)    # get metrics for each model for the current lag    # extract the data for the relevant models    # .values is returing a list of lists, so we need to flatten to get    # a single array holding all the collected metrics    # with the name of the model as the key    model_metrics_by_lag = {model: lag_data_sig_test.loc[lag_data['model'] == model, metrics].values.flatten() for model in all_models}    # perform calculations for each combination    # we already have a list that stores all combinations    # so we can just use that for now    lag_performance = []    for combo in all_combos:        # extract bothe models from combo tuples        model_1, model_2 = combo        # get the data for both models        model_1_metrics = model_metrics_by_lag[model_1]        model_2_metrics = model_metrics_by_lag[model_2]        # perform the kruskal-wallis test on each model's        # performance metrics for the given lag        h_statistic, p_value_kw = kruskal(model_1_metrics, model_2_metrics)        # perform the wilcoxon-signed rank test        # unlike in other comparisons, we encounter a few cases where all differences are zero        # in order to handle these cases, we need to adjust the zero_method to zsplit        statistic, p_value_ws = wilcoxon(model_1_metrics, model_2_metrics, zero_method='zsplit')        # store the result        result = {'kw_result': {'h_statistic': h_statistic, 'p_value_kw': p_value_kw},                  'ws': {'statistic': statistic, 'p_value_ws': p_value_ws},'models': {model_1, model_2}, 'lag': lag}        # append to performance list for current lag        # we will then use this to extract the lags        # which are statistically significant        lag_performance.append(result)        # append result to global list for further analysis        lag_sig_test_results.append(result)        # find significant results for the current lag    significant_results_by_lag.extend([result for result in lag_performance                                       if result['kw_result']['p_value_kw'] < 0.05 or result['kw_result']['p_value_kw'] < 0.05])# do final check looking strictly at the f1 statistic# this is by far the most relevant measure for these types of binary models# so if there is a significant difference, it will likely be captured here# we will first perform this generally across all lags# then do the same test across lags#---------------------------------## create helper function to calc f1 significancedef f1_general_significance_test(model_1, model_2):    """Function to test significance of       f1 values across all lags"""    # get the f1 scores for each model    model_1_f1_scores = summary_stats.loc[summary_stats['model'] == model_1, 'f1']    model_2_f1_scores = summary_stats.loc[summary_stats['model'] == model_2, 'f1']    # now perform the kruskal-wallis test    h_statistic, p_value_kw = kruskal(model_1_f1_scores, model_2_f1_scores)    # perform the wilcoxon-signed rank test    statistic, p_value_ws = wilcoxon(model_1_f1_scores, model_2_f1_scores, zero_method='pratt')    # return the values as dictionary    result = {'kw_result': {'h_statistic': h_statistic, 'p_value_kw': p_value_kw},              'ws': {'statistic': statistic, 'p_value_ws': p_value_ws},'models': {model_1, model_2}}    # return the result    return result    # initialize general f1 result listf1_general_results = [f1_general_significance_test(combo[0], combo[1]) for combo in all_combos]# extract the significant resultssignificant_general_f1_results = [result for result in f1_general_results                                  if result['kw_result']['p_value_kw'] < 0.05 or result['kw_result']['p_value_kw'] < 0.05]# loop and perform the same calculations based on lag# initialize lists to store the resultsall_f1_comp = []f1_sig_results = []# loop over and make comparisonfor lag, lag_data in lag_stats:    # get the data for the current lag    lag_data_sig_test_f1 = lag_stats.get_group(lag)    # get metrics for each model for the current lag    # extract the data for the relevant models    # .values is returing a list of lists, so we need to flatten to get    # a single array holding all the collected metrics    # with the name of the model as the key    model_metrics_by_lag_f1_test = {model: lag_data_sig_test_f1.loc[lag_data['model'] == model, 'f1'].values.flatten() for model in all_models}    # perform calculations for each combination    # we already have a list that stores all combinations    # so we can just use that for now    lag_performance_f1 = []    for combo in all_combos:        # extract bothe models from combo tuples        model_1, model_2 = combo        # get the data for both models        model_1_f1 = model_metrics_by_lag[model_1]        model_2_f1 = model_metrics_by_lag[model_2]        # perform the kruskal-wallis test on each model's        # performance metrics for the given lag        h_statistic, p_value = kruskal(model_1_f1, model_2_f1)        # store the result        result_f1 = {'h_statistic': h_statistic, 'p_value': p_value, 'models': {model_1, model_2}, 'lag': lag}        # now perform the kruskal-wallis test        h_statistic, p_value_kw = kruskal(model_1_f1, model_2_f1)        # return the values as dictionary        result_f1 = {'kw_result': {'h_statistic': h_statistic, 'p_value_kw': p_value_kw},'models': {model_1, model_2}}        # append to performance list for current lag        # we will then use this to extract the lags        # which are statistically significant        lag_performance_f1.append(result_f1)        # append result to global list for further analysis        all_f1_comp.append(result_f1)        # find significant results for the current lag    f1_sig_results.extend([result for result in lag_performance_f1                           if result['kw_result']['p_value_kw'] < 0.05])    # so far there seems to be no evidence that the ml models perform better than the# logit models, which throws water on the idea that ml models are better# perform one final test by looking at the averages of all stats for the ml models# and comparing them to the averages for the logit model# get the average scores for all ml modelsml_data = summary_stats.loc[summary_stats['model'] != 'logit', metrics]# get mean of each columnml_metric_means = ml_data.mean().values# get the logit model statslogit_stats = summary_stats.loc[summary_stats['model'] == 'logit', metrics].mean().values# compare the stats for the ml and logit modelsh_stat_ml_logit, p_value_ml_logit = kruskal(ml_metric_means, logit_stats)ws_stat_ml_logit, p_value_ml_logit_ws = wilcoxon(ml_metric_means, logit_stats, zero_method='pratt')# store resultsml_logit_comp = {'kw': {h_stat_ml_logit, p_value_ml_logit}, 'ws': {ws_stat_ml_logit, p_value_ml_logit_ws}}# looks like # export each result to an excel sheetfor lag, result in zip(lags, results):    # export to excel    result.to_excel(writer, sheet_name=f"{lag}_ranking_result")# close writerwriter.close()    